# LLM Agent Evaluation Framework

A lightweight evaluation framework for testing LLM-based agents with
dataset-driven benchmarks and automated scoring.

## Features
- JSONL-based task datasets
- Pluggable LLM backends (local Ollama)
- Exact-match and rubric-based evaluation
- Run-level metrics and traceable outputs

## Requirements
- Python 3.10+
- Ollama running locally

## Setup
```bash
pip install -e .